<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Teaching Humanistic Data Analysis | Ryan C. Cordell</title> <meta name="author" content="Ryan C. Cordell"/> <meta name="description" content="Book history, digital humanities, old newspapers, and information sciences "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üóûÔ∏è</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ryancordell.org/research/teachingHDA/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://ryancordell.org/"><span class="font-weight-bold">Ryan</span> C. Cordell</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">links</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="http://cv.ryancordell.org/" target="_blank" rel="noopener noreferrer">CV</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/statements/">Dossier Statements</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="http://viraltexts.org/" target="_blank" rel="noopener noreferrer">Viral Texts Project</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href=""></a><a rel="noopener noreferrer" href="https://bsky.app/profile/ryancordell.bsky.social" target="_blank">BlueSky</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Teaching Humanistic Data Analysis</h1> <p class="post-meta">May 6, 2019‚Ä¢ Ryan Cordell</p> <p class="post-tags"> <a href="/blog/2019"> <i class="fas fa-calendar fa-sm"></i> 2019 </a> </p> </header> <article class="post-content"> <p><em>The talk below was delivered on 2 May 2019 as part of Gale‚Äôs <a href="https://www.gale.com/intl/primary-sources/digital-humanities-at-the-british-library" target="_blank" rel="noopener noreferrer">Digital Humanities Day</a> at the British Library. An open-access volume of the day‚Äôs talks is supposed to appear in future; these are my unedited speaking notes, which are likely to be expanded and honed as the volume works through the editorial process.</em></p> <h2 id="introduction-humanities-data">Introduction: Humanities Data</h2> <p>Lorraine Daston and Peter Galison‚Äôs book <a href="https://mitpress.mit.edu/books/objectivity" target="_blank" rel="noopener noreferrer"><em>Objectivity</em></a> attempts to trace the emergence of objectivity as a concept, ideal, and moral framework for scientists during the nineteenth century. The book focuses primarily on shifting ideas about scientific images during the period. In the eighteenth and early nineteenth centuries, Daston and Galison argue, the scientific ideal was ‚Äútruth-to-nature,‚Äù in which particular specimens are primarily useful for the ways in which they reflect and help construct an ideal type: not this leaf, specifically, but this type of leaf. Under this regime scientific illustrations did not attempt to reconstruct individual, imperfect specimens, but instead to generalize from specimens and portray an ideal type.</p> <p>The ‚Äútruth-to-nature‚Äù framework changed, in part, because of the introduction of photography, which nudged scientists toward a new ideal, of mechanical objectivity. In early debates about the virtues of illustration versus photography, illustration was touted as superior to the relative primitivism of photography, as technologies such as drawing or engraving simply allowed finer detail than blurry nineteenth century photography could. Nevertheless, photography increasingly dominated scientific images over the course of the century because it was seen as less susceptible to manipulation, less dependent on the imagination of the artist (or, indeed, of the scientist). As Daston and Galison explain,</p> <blockquote> <p>It is this internal struggle to control the will that imparted to mechanical objectivity its high moral tone‚Ä¶One type of mechanical image, the photograph, became the emblem for all aspects of noninterventionist objectivity‚Ä¶This was not because the photograph was more obviously faithful to nature than handmade images‚Äîmany paintings bore a closer resemblance to their subject matter than early photographs, if only because they used color‚Äîbut because the camera apparently eliminated human agency. (187)</p> </blockquote> <p>For scientists increasingly worried that their own stubborn wills would sully the truth of their findings, mechanical means of image production offered a way out.</p> <p>Eventually, however, the camera also enabled views of nature impossible for the human eye alone. As the exposure time required for photography was reduced, for instance, it enabled new accounts of motion. Consider Eadweard Muybridge‚Äôs <a href="http://100photos.time.com/photos/eadweard-muybridge-horse-in-motion" target="_blank" rel="noopener noreferrer">famous photographs of racing horses</a>, made between 1878 and 1884, which established that horses do indeed bring all four hooves off the ground at certain moments during a gallop. In this and subsequent photo series, the technology of the photograph dramatically recontextualized scientists‚Äô conception of nature by allowing them to freeze motion and analyze its constituent ‚Äúparts‚Äù separately. In other words, these photographs convert a fluid motion, or a process, into data: discrete units of observation, measurement, or analysis.</p> <p>This moment in scientific history reminds me of a touchstone essay for those engaged in computational text analysis in the humanities. In <a href="http://dhdebates.gc.cuny.edu/debates/text/28" target="_blank" rel="noopener noreferrer">‚ÄúText: A Massively Addressable Object‚Äù</a>, Michael Witmore argues that ‚ÄúWhat distinguishes th[e digital] text object from others‚Äù is that</p> <blockquote> <p>it is <em>massively addressable at different levels of scale</em>. Addressable here means that one can query a position within the text at a certain level of abstraction‚Ä¶The book or physical instance, then, is <em>one of many levels of address</em>. Backing out into a larger population, we might take a genre of works to be the relevant level of address. Or we could talk about individual lines of print, all the nouns in every line, every third character in every third line. All this variation implies massive flexibility in levels of address. And more provocatively, when we create a digitized population of texts, our modes of address become more and more abstract: all concrete nouns in all the items in the collection, for example, or every item identified as a ‚ÄúHistory‚Äù by Heminges and Condell in the First Folio. Every level is a provisional unity: stable for the purposes of address but also stable because it is the object of address. Books are such provisional unities. So are all the proper names in the phone book.</p> </blockquote> <p>As Whitmore acknowledges, ‚ÄúPhysical texts were <em>already</em> massively addressable before they were ever digitized‚Äù through scholarly apparatus such as indices, concordances, or marginal notes. It is the flexibility of address that is new. Suddenly the book, or even the library, needn‚Äôt be converted into a new form to become data: the book or library can itself be data.</p> <p>Certainly other forms of humanistic data also existed before computation: consider the library card catalog or subject bibliography as forms of database. Like Muybridge‚Äôs photographs, however, the computer reshapes our relationship to text by making it radically separable and manipulable: a unity that can be reshaped into its constituent parts and at many levels, as Whitmore outlines. The primary objects of much humanistic inquiry‚Äîand text is only one of these, though it is what I know best and thus what I focus on here‚Äîcan be considered as discrete units of observation, measurement, or analysis.</p> <h2 id="toward-a-data-humanism">Toward a Data Humanism</h2> <p>This paper poses and begins to answer two interrelated questions: first, what precisely do we mean by ‚Äúdata analysis‚Äù as an area of instruction in humanities courses? and second, how might we distinguish humanistic data analysis from analogous methods in the hard and social sciences? In much the same way that Datson and Galison argue scientists turned toward machines in order to avoid the mistakes of human judgment, in some articulations of digital humanities, computation is invoked as a solution to problems of will that are quite familiar from decades of humanistic scholarship. We all know that the canon of English literature was shaped as much by the identities of its gatekeepers as by the inherent virtues of the books it includes. Feminist, postcolonial, and other theoretical schools have taught us that our biases limit our critical horizons and result in atrophied canons that do not adequately represent human identities or literatures.</p> <p>Methods such as distant reading, macroanalysis, or cultural analytics are sometimes proposed as methods that can bypass the fallible human wills that constructed existing canons through a species of mechanical objectivity. While human beings choose what to focus on for all kinds of reasons, including conscious or unconscious biases we would seek to mitigate, the computer will look for textual patterns unencumbered by those preconceptions. Under this vision of computational research, the machine is less susceptible to the social, political, or identity manipulations of canon formation.</p> <p>If we look closer, of course, we see that this position only sets the problem at one remove, in the humans writing the code rather than in the humans assembling the canon. As scholars such as <a href="https://safiyaunoble.com/" target="_blank" rel="noopener noreferrer">Safiya Umoja Noble</a>, <a href="https://mathbabe.org/" target="_blank" rel="noopener noreferrer">Cathy O‚ÄôNeil</a>, and <a href="https://virginia-eubanks.com/" target="_blank" rel="noopener noreferrer">Virginia Eubanks</a> have amply shown, computer programs always come laden with their creators‚Äô biases and oversights. Whether purposefully or inadvertently, software designed to create business or government efficiencies often reinscribes structural inequalities such as white supremacy, and as scholars <a href="https://muse.jhu.edu/article/613375/summary" target="_blank" rel="noopener noreferrer">such as Ben Fagan</a> have demonstrated, the same holds true for many mass digitization projects that through their selection processes reinscribe the dominance of mainstream cultures. Given this reality, I would suggest that we cannot hope to automate our way into a more just canon formation. Instead, we should advocate for a humanities data analysis that seeks primarily point our attention to the gaps in our digitization and critical practices. To my mind, the most compelling articulation of this potential remains <a href="http://lklein.com/2015/06/the-carework-and-codework-of-the-digital-humanities/" target="_blank" rel="noopener noreferrer">Lauren Klein‚Äôs description of topic modeling</a> as ‚Äúa technique that stirs the archive‚Äù and which, in an iterative relationship, can be in turn stirred by the archive (and as a side note, Klein‚Äôs book-in-progress with Katherine D‚ÄôIgnazio, <a href="https://bookbook.pubpub.org/data-feminism" target="_blank" rel="noopener noreferrer"><em>Data Feminism</em></a>, will be an essential touchstone in this area when complete).</p> <p>In Klein‚Äôs account, methods of humanistic data analysis allow researchers and students to articulate connections or paths through a collection that are not apparent through linear reading, while at the same time‚Äîand this is essential‚Äîthe contours of our collections shape the kinds of connections or paths available, such that ‚Äúdomain experts‚Ä¶must be able to probe the semantic associations that the model proposes.‚Äù In order to make sense of a topic model, the researcher must understand the collection upon which it was trained in order to articulate the set of possibilities available to the model and to notice what possibilities are missing.</p> <p>The primary outcomes of the computational analyses Klein describes are not statistical models‚Äîthough they might employ such models along the way‚Äîbut instead a speculative dialogue between researcher and data. We can trace a similar conception of humanistic data analysis in Stephen Ramsay‚Äôs 2011 book <a href="https://www.press.uillinois.edu/books/catalog/75tms2pw9780252036415.html" target="_blank" rel="noopener noreferrer"><em>Reading Machines</em></a>. Ramsay is best known for <a href="https://web.archive.org/web/20170426170232/http://stephenramsay.us/text/2011/01/08/whos-in-and-whos-out/" target="_blank" rel="noopener noreferrer">a provocative three-minute position paper</a> delivered at the 2011 MLA conference‚Äîin a roundtable, it must be remembered, in which all participants were asked to draft provocative position papers to spur discussion‚Äîin which he argued that digital humanities scholars should know how to code. In <em>Reading Machines</em>, however, Ramsay offers a deeply nuanced‚Äîand, frankly, more characteristic‚Äîvision of what might constitute an ‚Äúalgorithmic criticism‚Äù in literary studies. In particular, Ramsay rejects scientistic frameworks for computational work in favor of a dialogic framework:</p> <blockquote> <p>If text analysis is to participate in literary critical endeavor in some manner beyond fact-checking, it must endeavor to assist the critic in the unfolding of interpretive possibility‚Ä¶However far ranging a scientific debate might be, however varied the interpretations being offered, the assumption remains that there is a singular answer (or a singular set of answers) to the question at hand. Literary criticism has no such assumption. In the humanities the fecundity of any particular discussion is often judged precisely by the degree to which it offers ramified solutions to the problem at hand. We are not trying to solve Woolf. We are trying to ensure that discussion of <em>The Waves</em> continues (10, 15).</p> </blockquote> <p>In this passages Ramsay emphasizes the distinct hermeneutic underpinnings of humanistic work. In this paper I will follow Klein and Ramsay‚Äôs frameworks to argue for humanistic data analysis that is primarily exploratory, iterative, and dialogic, with a goal of directing our attention to unexpected places in and outside our digitized collections, while raising new questions about those collections and their absences.</p> <p>This ‚Äúdata humanism‚Äù is not entirely dissimilar the methods often gathered under the phrase ‚Äúdata science.‚Äù At least in the United States, new Masters programs in data science seem to pop up daily, alongside promises about the rewarding and well-compensated work that will follow such a credential. From the rhetoric alone, data science seems to be something like, but not quite identical to, computer science: programming is required, likely in a statistics-friendly language such as R, but the focus is not on building systems or writing software. Instead, the data scientist is adept at ‚Äújust in time‚Äù programming that explores economic, political, or civic datasets, identifying trends of immediate use to government, corporate, nonprofit, journalistic, or other actors. The key trait of the data scientist seems to be flexibility, as they work across domains and datasets seeking meaningful patterns. Though these qualities are also useful for humanities data analysis, I will propose that data <em>humanism</em> must differ, both in substance and methodology, from data analytics as it is imagined and practiced.</p> <p>Digital humanities scholars have been routinely accused of trying to turn literature, history, and related fields into branches of computer science. This charge is largely untrue. In particular, the interpretive aims of DH work are quite distinct from CS, and many claims that conflate the fields stem from mistaken assumptions about what computer scientists do. However, the ways we teach computing proficiency, the specific computational techniques we employ, and the ways we describe the findings of computational work have drawn too exclusively from corpus linguistics, data science, and related social sciences. We have largely failed to develop specifically humanistic approaches to computational data analysis, and the keen lack of such methods has become a particular hindrance to our field‚Äôs development. Seeking to simply add data science skills to humanities students misses the unique contributions our fields and our students might make to broader conversations about data and culture. The most important reason to imagine a distinct data humanism is that doing so in turn imagines students who will go into the world outside the academy and change it, rather than simply slotting into existing frameworks. I do not propose data humanism as opposed to data science, but as a complement and (occasional) corrective.</p> <h2 id="in-practice">In Practice</h2> <p>So what might data humanism look like in practice, and how do we inculcate its skills in our students? The remainder of this paper will draw on my experiences teaching data analysis to both undergraduate and graduate students in Northeastern University‚Äôs English Department, largely through courses such as <a href="https://f18rwda.ryancordell.org/" target="_blank" rel="noopener noreferrer">‚ÄúReading and Writing in the Digital Age‚Äù</a>, <a href="https://s19tot.ryancordell.org/" target="_blank" rel="noopener noreferrer">‚ÄúTechnologies of Text‚Äù</a>, <a href="http://s17hda.ryancordell.org/" target="_blank" rel="noopener noreferrer">‚ÄúHumanities Data Analysis‚Äù</a>, and <a href="https://s19rm.ryancordell.org/" target="_blank" rel="noopener noreferrer">‚ÄúReading Machines: Technology and the Book‚Äù</a>. These issues are also very much on my mind as I prepare to teach a version of <a href="http://www.dhsi.org/courses.php#DataAnalysis" target="_blank" rel="noopener noreferrer">‚ÄúHumanities Data Analysis‚Äù</a> at the 2019 Digital Humanities Summer Institute at the University of Victoria. The principles I draw from those classroom experiences evidence my own evolving sense of how a data-rich humanities pedagogy might be structured and what goals it might seek to meet, which is to say I will reflect as much on approaches that have failed as on those that have succeeded. The <em>ideals</em> I outline are not ones I believe my teaching has yet met, though I seek each semester to work closer to them. In outlining these ideals, I also do not mean to imply that no one in computer science, data science, digital humanities, or cultural analytics follows these practices. Certainly many do, I my recommendations are nearly all grounded in examples (which I will try to cite fairly).</p> <h3 id="1-start-with-creativity">1. Start with Creativity</h3> <p>For many years, I would begin the programming units in my classes with text analysis: word counts, keywords in context, ngrams, topic modeling. After spending several weeks (in a single unit within a larger class) or most of the semester (in an HDA focused class) on these activities, we might devote a final week to a more creative engagement with programming, such as building a poetic Twitter bot. In recent years, however, I have become convinced that this is precisely the wrong way to introduce humanities students to programming or data. This past year, I reversed the order of operations, and I think all of my classes benefited immensely.</p> <p>In my ‚ÄúReading and Writing in the Digital Age‚Äù course, we began our unit on data with Giorgia Lupi and Stefanie Posavec‚Äôs project <a href="http://www.dear-data.com/theproject" target="_blank" rel="noopener noreferrer"><em>Dear Data</em></a>, which began as a website and became a beautiful book. Lupi and Posavec spent a year recording a different aspect of their daily lives each week, creating a new way to visualize that data on a postcard, and mailing the postcards to each other; the book compiles these 104 experiments and presents them beautifully. We began here because I wanted my students first to understand that data is a bigger category than digital data, and that the suites of visualization tools built into existing computer programs include only a handful from the infinite possibilities for data visualization. Students were <a href="https://f18rwda.ryancordell.org/assignments/dear-my-data.html" target="_blank" rel="noopener noreferrer">then charged</a> to spend a week recording and then visualizing some aspect of their own data, reflecting both on what they learned from the data, the choices they made in representing it, and the ways those representational choices shaped what they could (and couldn‚Äôt) learn about the data. An exercise such as this prepares students to encounter computational data work from a productively critical perspective.</p> <p>This year I also began the programming units in two courses, my undergraduate ‚ÄúTechnologies of Text‚Äù and my graduate ‚ÄúReading Machines,‚Äù with the Twitter bot assignment that used to serve as the cap to this unit. On the whole the exercise is pretty simple: students choose a poem or other short text and then build a program that will substitute words of the appropriate part of speech into the chosen poem: think of the children‚Äôs game ‚ÄúMad Libs.‚Äù The results are sometimes nonsensical, sometimes hilarious, and sometimes oddly profound. Practically, students learn a number of important skills through this lesson, including how to manipulate text strings in R, how to query web services using an application programming interface (API), and how to output the results of an R program back to a web service such as Twitter. Most importantly, however, they learn these skills through a process that is generative and creative, and which leverages their existing skills, such as the ability to understand and analyze a poem: at least well enough to know what substitutions are likely to ‚Äúwork‚Äù and be funny. Though simple, this exercise fits into a long tradition of cut-up and experimental poetry with which students are familiar, and it helps them understand programming from the outset as something more creative and expressive than they might have imagined.</p> <p>The way we introduce data analysis matters. If you survey publicly-available ‚Äúlearn to code‚Äù courses on sites such as Coursera or Codeacademy, you will notice that, regardless of language, they all begin in nearly the same way, teaching students how to do math. I do not want to imply that math does not enter into humanities data analysis‚Äîthough I will discuss this further below‚Äîbut I will argue that, given the huge range of tasks to which programming can be put in 2019, beginning in this way is not necessary and can throw up immediate barriers to humanities students who perhaps see themselves as not skilled at or interested in mathematics.</p> <p>Most current programs for teaching coding or data analysis presume that the endeavor must be rooted in computer science, but I follow Annette Vee in seeking to decouple programming as a practice from computer science as a discipline, to the benefit of both. In <a href="https://mitpress.mit.edu/books/coding-literacy" target="_blank" rel="noopener noreferrer">her book <em>Coding Literacy</em></a>, Vee argues that in much of the world today ‚Äúcomputer code is infrastructural,‚Äù a literacy in the ways it is valued and deployed: ‚Äúlayered over and under the technology of writing, computer code now structures much of our contemporary communications‚Äù (3).</p> <blockquote> <p>When we consider programming a mode of written communication, it is no longer bounded by the field of computer science. Its roots are no longer solely in math, engineering, and science; they include written communication as well. Decoupling programming from CS not only helps us understand programming as communication but also frees CS from being overly identified with just one of their practices‚Ä¶neither programming nor CS is well served by the idea that they map perfectly onto each other.[@vee2017, pg. 41]</p> </blockquote> <p>Vee shows in her book that ‚Äúprogramming never was a domain exclusively for specialists‚Äù while advocating that we consider programming a literacy that can be applied in many ways and in many domains. This is also my goal, and is a primary reason that I seek to show students that a humanistic approach to data can have distinct aims from from the outset. To expand out a bit: we need new pedagogical resources to teach data analysis for humanities students that begins with questions, data, and programming tasks that will resonate with those students.</p> <h3 id="2-teach-using-domain-specific-data">2. Teach Using Domain-Specific Data</h3> <p>My next recommendation is straightforward: when teaching students humanities data analysis, we should use humanities data. This point might seem obvious, but it has not been so in practice, in part because it can be harder to practice than it should be. Insisting on teaching with humanistic data leads inevitably to a complementary idea, that we should not shy away from data that is complex and messy. Computer science and data science are often taught using tidy datasets that are designed, quite literally, to help students gain proficiency with particular methods. I think of the <code class="language-plaintext highlighter-rouge">mtcars</code> data that is built into the R programming environment. It is a relatively small, well-structured table of information about various car models, including details such as their miles per gallon efficiency. The dataset was extracted from the 1974 issue of <em>Motortrend</em> magazine, and is designed to be a resource for learning how to use R‚Äôs mathematical or plotting functions, and thus is invoked in tutorial after tutorial teaching just these functions. The other most popular built-in datasets for R include <code class="language-plaintext highlighter-rouge">iris</code>, which includes various measurements for 50 flowers in the iris family; <code class="language-plaintext highlighter-rouge">ToothGrowth</code>, which contains the results of an experiment studying the effect of vitamin C on tooth growth; or <code class="language-plaintext highlighter-rouge">USArrests</code>, which includes statistics about violent crime in the US.</p> <p>There are a number of problems with defaulting to such datasets when teaching humanities students. First, our students often find it difficult to extrapolate from data about cars or tooth growth to the domains they care about. This is especially true for students coming to data analysis with some hesitancy or trepidation, as many stock training datasets available resonate, teleologically, as corporate or governmental. To state an even stronger version of this point, these datasets look and feel like what our students think data should look and feel like: which is to say, they look and feel like information from domains far removed from humanistic inquiry, and they alienate students rather than welcoming them in.</p> <p>Second, however, such datasets are ontologically distinct from humanities data, insofar as data drawn from literary, historical, and related domains is defined, at least in part, by its complexity and messiness. This is in fact why many computer science professors are drawn to our data for their own research: it is intriguingly difficult. A few years ago, I co-taught the course ‚ÄúBostonography‚Äù for computer science undergraduates at Northeastern. Our students in that course repeatedly expressed how frustrated but exhilarated they were to be working with what they called ‚Äúreal data‚Äù‚Äîwhich is to say, historical data related to the city of Boston‚Äîrather than the canned datasets they encountered in their CS classes. The data in our class, we heard, was uniquely challenging but more rewarding than the dull stuff they encountered again and and again. Humanities students need to work with this challenging humanities data from the beginning.</p> <p>Unfortunately it is not self-evident how to teach using humanities data‚Äîso much so that perhaps the most common complaint among those teaching such classes is the lack of relatively small, teachable data sets in literary studies, history, or related fields. Many of the datasets one encounters in the field‚Äôs literature are simply too large for a group of students to work with simultaneously in a classroom setting, and in fact can be frustrating even outside the classroom when it takes many minutes‚Äîor much longer‚Äîeach time a student has to learn that a line of code does not work as it should. The iteration required for learning programming‚Äîrun the code, see if it works, revise, run it again‚Äîcannot happen with very big data, which is why fields such as CS and data science have developed the teaching datasets I described earlier. As more people seek to cultivate data humanism in students, we will need to cultivate also manageable, but still interestingly messy, datasets for use in classrooms.</p> <p>My own research largely centers on historical newspapers, and I have developed some unexpectedly generative teaching exercises around the Library of Congress‚Äô <a href="https://chroniclingamerica.loc.gov/search/titles/" target="_blank" rel="noopener noreferrer">U.S. Newspaper Directory</a>. This is not their collection of digitized newspapers, which is far too large for classroom work, but instead their index of metadata about all known newspapers founded in the United States between 1690 and the present. Like R‚Äôs <code class="language-plaintext highlighter-rouge">cars</code> dataset, there are many natural experiments to be run here to learn how to compare categories or plot: how many weeklies versus dailies were founded in the 1870s? Which states had the most newspapers at the turn of the century? Can we analyze or visualize the most popular words used in the titles of new newspapers over decades, and what might such analyses tell us about the changing ways editors and readers conceptualized the newspaper across time? Not all of my students are newspaper researchers, but because this data is about something closer to their interests, they know the kinds of questions we might ask about it, and these investigations are more easily transferred into their own domains.</p> <h3 id="3-foreground-corpus-over-method">3. Foreground Corpus Over Method</h3> <p>Asking questions about the data we use in our classrooms leads directly to my next recommendation, which is that humanities data analysis requires that teaching our corpora or datasets precede teaching methods. In her book <a href="https://www.press.umich.edu/8784777/world_of_fiction" target="_blank" rel="noopener noreferrer"><em>A World of Fiction</em></a>, Katerine Bode draws on her book historical expertise to critique the ways many computational literary scholars approach the data at the heart of their analyses. In particular, Bode argues that</p> <blockquote> <p>in their literary-historical work both [Franco] Moretti and [Matthew L.] Jockers present literary data and digital collections as pre-critical, stable, and self-evident. In conceiving data and computation as providing direct and comprehensive access to the literary-historical record, they deny the critical and interpretive activities that construct that data and digital record and make them available for analysis (20).</p> </blockquote> <p>Bode does not claim distant reading scholars fail to disclose the corpora from which their conclusions are drawn, but instead that they often do not adequately describe the those corpora: what they include, what they do not. In this way, Bode argues, distant reading looks a lot like new critical close reading, in that the specific material circumstances of the text at hand‚Äîwhether that text is a single Keats poem or the Hathitrust library‚Äîare discounted or overlooked in favor of an idealized notion of ‚Äúthe text‚Äù or ‚Äúthe archive.‚Äù</p> <p>Bode‚Äôs claim extends critiques by scholars such as Johanna Drucker, <a href="http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html" target="_blank" rel="noopener noreferrer">who argues that</a> the very word <em>data</em> implies that the information it labels is ‚Äúa ‚Äògiven‚Äô able to be recorded and observed‚Äù or simply ‚Äúa natural representation of pre-existing fact.‚Äù While I am not <em>quite</em> advocating with Drucker that we replace the word ‚Äúdata‚Äù with ‚Äúcapta‚Äù‚Äîbecause ‚Äú<em>capta</em> is ‚Äòtaken‚Äô actively‚Äù‚ÄîI would concur that a data humanism must begin with discussions of data construction rather than data analysis. To make claims from the Library of Congress‚Äô Chronicling America newspaper archive, as we do in the <a href="https://viraltexts.org/" target="_blank" rel="noopener noreferrer"><em>Viral Texts</em></a> project, requires understanding how its newspapers were compiled through the National Digital Newspaper Program, which lays out specific selection guidelines for states that participate. These guidelines include ideas of ‚Äúrepresentativeness‚Äù around circulation and influence that ultimately shape the kinds of newspapers included, and the kinds excluded, from the overall collection.</p> <p>Returning to classroom conversations about the Library of Congress‚Äô Newspaper Directory, I would note that conversations about what we might learn analyzing it lead naturally to rich conversations about the construction of the dataset itself. Who compiled these lists? How did they obtain the information about the newspapers? What information is not included? For instance, does the data include anything about the communities that various newspapers served? If we analyze the most popular words used in titles, what contours of the newspaper landscape in those years are we necessarily excluding? A humanistic approach to data analysis must foreground such conversations and teach students how to probe the intellectual, social, and political underpinnings of each dataset or corpus they encounter.</p> <h3 id="4-foreground-mindset-over-method">4. Foreground Mindset Over Method</h3> <p>The courses I draw from in this paper largely teach humanities data analysis through coding in the programming language R. I would not argue that coding is the only way that effective humanities data analysis might be taught or practiced, in large part because this raises the barrier of entry too high. This is an area of inquiry that needs more voices, not fewer. There are many good tools out there for performing most common analysis tasks, and one can be thoughtful about all of the issues I have thus far outlined in graphical user interfaces (GUIs) as easily as in a programming language. And to be frank: my own work in this area began using GUIs and applications, which helped me cultivate approaches to questions that eventually required me to cultivate programming abilities. That early work was no less valid than the work I do now.</p> <p>However, I will outline the reasons that I have moved more and more toward approaching humanities data analysis in my classes through programming rather than using common DH applications. In fact I should immediately modify that to say that I teach humanities data analysis through workbooks, which are in a way an intermediary form between an application and pure programming. In R such workshops are composed as RMD files, and they weave together prose with executable blocks of code. In this form I can include relatively complex chunks of code, even in the first days that we work on programming, so we can move immediately to applications that will feel more consequential to students than printing <code class="language-plaintext highlighter-rouge">Hello, World</code> might. When looking at an RMD file like this, students might not understand each line. In contrast to a GUI, however, in this form each line is available for inspection, and good deal of the work we do together, at least initially, consists of running a block of code, looking at the results, and then working back through the code to understand what precisely it did step-by-step.</p> <p>I follow this procedure for a few reasons. First, I largely concur <a href="http://dhdebates.gc.cuny.edu/debates/text/99" target="_blank" rel="noopener noreferrer">with my colleague, historian Ben Schmidt</a>, that ‚Äúdigital humanists do not need to understand algorithms <em>at all</em>,‚Äù insofar as understanding means grasping precisely what each greek letter in an algorithm‚Äôs equations stands for or being able to precisely reproduce the underlying math. What humanists do need, Schmidt continues, is ‚Äúto understand the transformations that algorithms attempt to bring about.‚Äù If students are to use computational techniques responsibly, they need to understand what happens to text or tabular data to move from a set of, say, novels to a vector space model. What assumptions are baked into the method? What variables can be controlled, and what happens when they are changed. It is possible to do this work within a GUI. Gale‚Äôs own Digital Scholars Lab is quite robust and good at foregrounding explanations of the methods it includes and the variables that researchers can control within it.</p> <p>I do not wish to pretend that working in a programming language like R lays everything bare, given that many elements of particular algorithms are packaged within functions that can be run with little understanding. For me, however, working through the code helps students understand a little more fully the processes through which transformation happens, to see each major step in a given process, and to begin asking questions about how particular functions operate. My workbooks typically include chunks of prewritten code as well as instructions about lines that students can manipulate, as well as empty code blocks they can use to copy, paste, and modify the code I have given them. This kind of tinkering introduces a sense of code as a medium‚Äîit is literally text that can be copied, pasted, edited‚Äîand opens up to later workbooks in which students have more direct agency, as well as independent projects in which students ask questions about their own data.</p> <p>Here‚Äôs an important point to make about all of the pedagogy I have been describing: I cannot turn students into proficient, independent programmers in a four-week unit, or even in a full semester class, and that is not my goal. Nor is my goal to bring them to expertise in any particular method: e.g. classification or topic modeling. What I am trying to cultivate is a mindset for approaching data, exploring it, figuring out what questions computation might help answer about it, and then determining what methods might help answer those questions. Ultimately I want them to begin to understand how to outline a series of steps‚Äîan algorithm, in a way, though not expressed mathematically‚Äîthat would allow them to transform the data from its original form into a form pertinent to the question at hand. To actually make that process happen likely would require more research and potentially collaboration. But I would argue the hardest skill for a humanities student to acquire is programmatic thinking, not programming.</p> <p>A deeply humanistic, exploratory approach to data will not only result in students able to use methods developed for computer science or social science on humanities data, but also will result in the need for new methods required by our materials. This, I would argue, is an area largely unexplored in digital humanities or adjacent fields. We have seen quite sophisticated projects that apply methods such as vector space analysis or topic modeling to novels or historical newspapers, but we have not seen new methods emerging from and specifically for historical, literary, or related data. But this is the future I want to teach toward. One reason we need humanistic data analysis is that we cannot cede either the maintenance and algorithmic framing of digital cultural heritage entirely to engineers or, with all deference to our hosts today, to corporations. Certainly products such as <a href="https://www.gale.com/intl/primary-sources/digital-scholar-lab" target="_blank" rel="noopener noreferrer">Gale‚Äôs Digital Scholar Lab</a> can be useful starting points for research within their collections and to teach the affordances of the methods they incorporate. Gale‚Äôs DSL is particularly powerful and flexible; I have been genuinely impressed with its capabilities.</p> <p>Nevertheless it remains true that no standard suite of analytical tools, however thoughtfully developed, will capture the extent of what we might hope to learn from a historical, literary, or social dataset, and one of the great challenges of humanistic work is that the questions of interest are difficult to generalize or predict well enough in advance to develop a generic tool that will cover them. Teaching students using only out of the box tools will limit their possibilities. In addition, I teach knowing that the majority of my students are not heading toward jobs at well resourced universities that are likely to subscribe to the databases that make a product like the DSL useful. The majority of them are heading toward work in the much broader field of US universities and colleges or into work in the private sector. If these students are to continue data-rich humanistic work‚Äîand, essentially, train their own students to do the same‚Äîthey will do so primarily using open access tools‚Äîincluding both GUIs and programming languages‚Äîand open access data. My students need to understand the basic underpinnings of such tools and be able to flexibility adapt to different institutional situations and, if necessary, know how to begin assembling their own suite of tools for the research and teaching they need to do.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Ryan C. Cordell. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>
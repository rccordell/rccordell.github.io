<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>The Scissors, the Paste-Pot, and the Large Language Model | Ryan C. Cordell</title> <meta name="author" content="Ryan C. Cordell"/> <meta name="description" content="Book history, digital humanities, old newspapers, and information sciences "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸ—ï¸</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ryancordell.org/research/scissors-paste-LLMs"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://ryancordell.org/"><span class="font-weight-bold">Ryan</span> C. Cordell</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">links</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="http://cv.ryancordell.org/" target="_blank" rel="noopener noreferrer">CV</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/statements/">Dossier Statements</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="http://viraltexts.org/" target="_blank" rel="noopener noreferrer">Viral Texts Project</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href=""></a><a rel="noopener noreferrer" href="https://bsky.app/profile/ryancordell.bsky.social" target="_blank">BlueSky</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Scissors, the Paste-Pot, and the Large Language Model</h1> <p class="post-meta">May 23, 2024â€¢ Ryan Cordell</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a> </p> </header> <article class="post-content"> <p>The text below was developed over a few speaking engagements this past year, including <a href="https://uicb.uiowa.edu/event/123436/0" target="_blank" rel="noopener noreferrer">UICBâ€™s Annual Nancy Brownell Lecture on the History of the Book</a>, a talk for <a href="https://www.fu-berlin.de/sites/dhc/programme/termine/2024-05-15_D_HCL-Cordell.html" target="_blank" rel="noopener noreferrer">Freie UniversitÃ¤tâ€™s Dahlem Humanities Center</a> and the <a href="https://ies.sas.ac.uk/events/bloomsbury-chapter-stevenson-lecture-2024" target="_blank" rel="noopener noreferrer">Bloomsbury Chapter Stevenson Lecture</a>. This research and my previous post, <a href="https://ryancordell.org/research/aibibliography/">â€œToward a Bibliography of AI Systems,â€</a> are both part of new work seeking to apply bibliographical and book historical insights to generative AI, and will be developed further toward more formal publication.</p> <h3 id="i-introduction">I. Introduction</h3> <p><a href="https://github.com/ViralTexts/gtr-clusters/blob/main/clusters/18600105-sn84026845-WantedAPrinter.md" target="_blank" rel="noopener noreferrer"><img style="float: right; padding: 30px;" src="https://camo.githubusercontent.com/6a752b97587531edaf531338d06363d25928d6bee2cecafebf9ede2aefcb194f/68747470733a2f2f6368726f6e69636c696e67616d65726963612e6c6f632e676f762f696969662f322f7776755f617573747269615f766572303125324664617461253246736e3834303236383435253246303032303231393036353025324631383630303130353031253246303031352e6a70322f7063743a35372e3136363730302c35332e3933313438352c31342e3036353334352c32392e3636393635372f213630302c3630302f302f64656661756c742e6a7067"></a></p> <p>â€œWantedâ€”a printer says a contemporary. Wantedâ€”a mechanical curiosity, with brain and fingers; a thing that will set up so many type a dayâ€”a machine that will think and act, but still a machine; a being who takes the most systematic and monotonous drudgeryâ€”yet one that the ingenuity of man has never supplanted, mechanically; thatâ€™s a printer.â€ This article, which imagines a printer as an intelligent machine, <a href="https://github.com/ViralTexts/gtr-clusters/blob/main/clusters/18600105-sn84026845-WantedAPrinter.md" target="_blank" rel="noopener noreferrer">was reprinted in at least 91 newspapers</a> across the United States between 1860 and 1892. In the US at this time, newspaper content was explicitly not protected by intellectual property law, and editors swapped papers for common use through what were called â€œexchanges.â€ In the United States, editors took advantage of favorable postal rates to trade their papers with each other for reciprocal use.Â Compositors filled daily, weekly, or more occasional issues with selections from the other newspapers on their exchange lists, as well as from the magazines, books, and other media that their editors sifted through for content.Â The hybridity of the nineteenth-century paper reflects their diverse sources. A piece like â€œWantedâ€”A Printerâ€ jostled for attention on the same page as short or serialized fiction; poetry, alongside hard news; travel and general interest pieces; practical information and advice; jokes, anecdotes, and miscellaneous facts; domestic, philosophical, and spiritual advice; and opinion alongside sometimes trite, sometimes profound aphorisms. Like a twenty-first century social media feed, the newspaper was a single medium for nearly every genre of content, often presented without evident organization.</p> <p>This text is typical of the selections we discover through the <em>Viral Texts</em> project, which I co-founded with David Smith at Northeastern University eight years ago and continue to work on at the University of Illinois. I will not belabor a discussion of our text mining methods today, but for those curious we link to several articles on our website at <a href="https://viraltexts.org" target="_blank" rel="noopener noreferrer">https://viraltexts.org</a> that describe these methods in detail, as does chapter two of our book in progress, which is <a href="https://manifold.umn.edu/projects/going-the-rounds" target="_blank" rel="noopener noreferrer">available in draft form from the University of Minnesota Press</a>. In brief, we have developed a suite of tools for fuzzy duplicate-text detection to identify clusters of likely-reprinted texts in digitized collections of historical newspapers. Growing from early experiments on pre-Civil War American newspapers held in the Library of Congressâ€™ <a href="https://chroniclingamerica.loc.gov/" target="_blank" rel="noopener noreferrer">Chronicling America</a> collection, <em>Viral Texts</em> now mines data from more than 20 large-scale historical periodicals archives, including papers from North America, Europe, and Australia and from the nineteenth and early-twentieth centuries. We use this data to explore different facets of nineteenth-century periodical culture, from the popular genres and authors that primarily circulated through newspaper reprinting, to the topics that generated the most discourse in the paper, to the ways reprinting can help us understand the flow of information across the country in the period.</p> <p>In this talk, I seek to connect this messy textual ecosystem to twenty-first textual technologies such as large language models, such as ChatGPT. <a href="https://doi.org/10.1353/bh.2023.a910951" target="_blank" rel="noopener noreferrer">In a 2023 <em>Book History</em> article</a>, Sarah Bull links the rise of â€œbook-makingâ€ in the nineteenth century, as a category separate from authorship, to â€œthe birth of the content generatorâ€ and, eventually, the ideas of textual content that underlie twenty-first century technologies like LLMs. By separating â€œtwo discrete categories of labor, the intellectual/creative and the bodily/mechanical,â€ Bull argues, nineteenth-century writers, editors, and publishers â€œpopularized the idea that all text and anything that could be rendered into text was contentâ€”alienable filler for a medium, in this case, the bookâ€”and that this material could be generated pretty much automatically.â€<sup id="fnref:Bull" role="doc-noteref"><a href="#fn:Bull" class="footnote" rel="footnote">1</a></sup> I will argue that the newspaper exchange system offers a similar analog, as a method of textual production founded on the selection, adaptation, and recontextualization of existing prose and poetry.</p> <p>As most newspapers were produced in small, local print shops by a few compositors and editors, that text was processed by the very â€œmachine that will think and act, but still a machineâ€ cited in my opening example. If we consider the full network of papers that constituted the exchange systemâ€”and the millions of densely-composed pages being exchanged through it each yearâ€”the nineteenth-century newspaper constituted perhaps the largest text generation platform in human history, and it offers both lessons and warnings for our current discussions of large language models. Like LLMs, nineteenth-century newspapers blended original and unoriginal writing in ways that were difficut for their readers to immediately apprehend, and that reality led to similarâ€”and warrantedâ€”anxieties about veracity, attribution, and reliability to what we face today.</p> <h3 id="ii-scissors--paste-pots">II. Scissors &amp; Paste Pots</h3> <p>Nineteenth-century newspaper editors frequently discussed their exchanges and the importance of reprinting to their medium. For instance, on <a href="https://newspapers.digitalnc.org/lccn/sn84024516/1888-06-06/ed-1/seq-2/" target="_blank" rel="noopener noreferrer">June 6, 1888</a>, the editor of Raleigh, North Carolinaâ€™s <em>News &amp; Observer</em> praised the new woodcut adorning the front page of a neighboring paper from Durham. â€œWe have the <em>Daily Tobacco Plant</em>,â€ they wrote, â€œwith a beautifully engraved colored head piece in which appropriately appear the scissors and the paste pot and which makes it the handsomest daily in America, so far as we know, or for that matter the world.â€ <a href="https://newspapers.digitalnc.org/lccn/sn91068306/1888-06-05/ed-1/seq-1/" target="_blank" rel="noopener noreferrer">Looking at the <em>Daily Tobacco Plant</em></a>, we see the details praised by the <em>News &amp; Observer</em>: to the left, the editorâ€™s ink-pot and quill sit atop a pile labeled â€œclippingsâ€; the editorâ€™s scissors run across the page, their handle looped through the capital â€œDâ€ in â€œDailyâ€; while the paste-pot sits on the other side, waiting to paste up chosen clippings, so the paperâ€™s compositors can set them in type for reprinting. By noting that these icons â€œso appropriately appearâ€ as the headpiece for a daily newspaper, the <em>News &amp; Observer</em> highlights the material reality of newspaper production across much of the US during the period, which comprised primarily reprinted material from other newspapers, rather than original writing, and were lauded as much for canny selection as for literary skill.</p> <p>In the <a href="https://viraltexts.org" target="_blank" rel="noopener noreferrer">Viral Texts Project</a>, we take our websiteâ€™s own head piece from an illustration that appeared <a href="https://babel.hathitrust.org/cgi/pt?id=mdp.39015030616430&amp;seq=65" target="_blank" rel="noopener noreferrer">in <em>Harperâ€™s Weekly</em> in January of 1874</a>, which depicts the editor of the fictional <em>Podonk Weekly Bugle</em> considering whether to accept two chickens as payment for a yearâ€™s subscription when, as the accompanying article notes, â€œpayment of a little hard cashâ€ would be more useful. While it is condescending to its subjectâ€”as the title <em>Podonk Weekly Bugle</em> immediately suggestsâ€”the article nonetheless praises the small, rural papers that made up the majority of the US newspaper network and were â€œthe only means by which many of [their] readers receive intelligence of what is going on in the great world outside.â€ In the illustration, we see again the scissors and paste pot, on the editorâ€™s desk, sitting atop clippings from other newspapers that he is considering for inclusion in the <em>Bugle</em>. In his garbage can are clippings that did not make the cutâ€”pun intendedâ€”while behind the subscriber we see a compositor using a pasted-up clipping to set type to reprint a chosen selection.</p> <p><a href="https://chroniclingamerica.loc.gov/lccn/sn84026601/1868-12-02/ed-1/seq-1/" target="_blank" rel="noopener noreferrer"><img style="float: right; padding: 30px; width: 40%" src="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/Screenshot%202024-05-12%20at%2012.49.41.png?raw=true"></a></p> <p>In <a href="https://chroniclingamerica.loc.gov/lccn/sn84026601/1868-12-02/ed-1/seq-1/" target="_blank" rel="noopener noreferrer">a popular selection</a> that made the rounds in the 1860s, often called out as from â€œan exchange,â€ an â€œeditor of a country newspaperâ€ rebuffs charges of poverty by declaring, among other things, â€œWe have a good office, a paste-pot.â€ <a href="https://chroniclingamerica.loc.gov/lccn/sn85025007/1847-08-18/ed-1/seq-4" target="_blank" rel="noopener noreferrer">Another selection</a> provided a recipe for making paste, and noted â€œNext to scissors, paste is an invaluable editorial assistantâ€ before praising the creator of this new recipe as â€œa Godfrey, a Franklin, a Fulton, a Davy, or a Morse.â€ While certainly some contemporaries criticized â€œscissors and pasteâ€ journalism, the frequency with which practitioners highlightedâ€”even richly illustratedâ€”the scissors and the paste pot signal they valued selection and aggregation as distinct editorial and literary practices. Reprinting was certainly widespread. Using our reprinting data from the <em>Viral Texts</em> project, <a href="https://manifold.umn.edu/read/editing-a-paper/section/fc0597a3-5fe1-439c-86f8-0e47e8a55208" target="_blank" rel="noopener noreferrer">we estimate that papers in the period</a> averaged more than 50% reprinted content, with some considerably more or less, and this is a conservative estimate based only on detected reprints.</p> <p>Nineteenth-century editorsâ€™ attitude toward text reuse is exemplified in <a href="https://chroniclingamerica.loc.gov/lccn/sn84038206/1890-12-27/ed-1/seq-1/" target="_blank" rel="noopener noreferrer">a selection that circulated in the last decade of the century</a>, though often abbreviated from the version I cite here, which insists that â€œan editorâ€™s selections from his contemporariesâ€ are â€œquite often the best test of his editorial ability, and that the function of his scissors are not merely to fill up vacant spaces, but to reproduce the brightest and best thoughtsâ€¦from all sources at the editorâ€™s command.â€ While noting that sloppy or lazy selection will produce â€œa stupid issue,â€ this piece claims that just as often â€œthe editor opens his exchanges, and finds a feast for eyes, heart and soulâ€¦that his space is inadequate to contain.â€ This piece ends by insisting â€œa newspaperâ€™s real value is not the amount of original matter it contains, but the average quality of all the matter appearing in its columns whether original or selected.â€ Looking at reprints of this piece, we see another key element of newspaper exchanges; texts were not only copied verbatim, but were modified as they circulated. Sometimes texts were shortened, and only particular sections circulated widely. Details were changed to fit particular a paperâ€™s geographic location, audience, political stance, or special interests. Material might be added to contextualize or editorialize.</p> <h3 id="iii-large-language-models">III. Large Language Models</h3> <p>We can see in the nineteenth-century newspaper exchanges a massive system for recycling and remediating culture. I do not wish to slip into hyperbole or anachronism, and will not claim historical newspapers as a precise analogue for twenty-first century AI or large language models. But it is striking how often metaphors drawn from earlier media appear in our attempts to understand and explain these new technologies. The most famous critique of LLMs, <a href="https://dl.acm.org/doi/10.1145/3442188.3445922" target="_blank" rel="noopener noreferrer">Bender <em>et al</em>â€™s â€œOn the Dangers of Stochastic Parrots,â€</a> describes an LLM as â€œa system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data.â€<sup id="fnref:Bender" role="doc-noteref"><a href="#fn:Bender" class="footnote" rel="footnote">2</a></sup> <a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web" target="_blank" rel="noopener noreferrer">Ted Chiang turns</a> to more recent copying technologies, comparing ChatGPT to a model of Xerox photocopiers that stored data in a compressed JPEG format, which subtly blurred imagesâ€”a effect magnified when copies were made of copies.<sup id="fnref:Chiang" role="doc-noteref"><a href="#fn:Chiang" class="footnote" rel="footnote">3</a></sup> A <a href="https://twitter.com/CosmoWenman/status/1568032424835297280" target="_blank" rel="noopener noreferrer">widely-shared and cited 2022 tweet</a> from Cosmo Wenmanâ€”who describes himself as an artist and â€œopen access activistâ€â€”looks even further back to characterize AI image synthesis as:</p> <blockquote> <p>like movable type for visual concepts. It is going to facilitate a radical expansion and diffusion of the power to communicate with visual rhetoric unlike anything the world has ever seen.</p> </blockquote> <p>Whether we are talking about ChatGPT for written language or something like Stable Diffusion for images, such arguments emphasize how these systems allow users to abstract an overall idea into discrete, atomized concepts that can be iteratively combined, like sorts from a typecase. To generate this image, for instance, we might imagine that I drew from my conceptual typecase the idea of â€œa woodcut,â€ â€œa robot,â€ â€œsetting movable type,â€ and â€œin a historical print shopâ€ to produce this hybrid image. The metaphor claims these systems make concepts, rather than the alphabet, almost infinitely recombinable.</p> <p><a href="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/0_A%20humanoid%20robot%20setting%20type%20in%20a%20print%20shop%2C%20in%20_esrgan-v1-x2plus-2.png" target="_blank" rel="noopener noreferrer"><img style="align: center; padding: 30px; width:100%" src="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/0_A%20humanoid%20robot%20setting%20type%20in%20a%20print%20shop,%20in%20_esrgan-v1-x2plus-2.png?raw=true"></a></p> <p>Importantly, large language models are do not work like simpler autocomplete algorithms, which suggest the next word in a string based on the probability of one word following another. LLMs include also â€œattention,â€ which means these models are trained to understand wordsâ€™ place within much longer strings of textâ€”what literary scholars might call contextâ€”so they can, <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/" target="_blank" rel="noopener noreferrer">as Stephen Wolfram explains</a>, do things like â€œcaptur[e] the way, say, verbs can refer to nouns that appear many words before them in a sentence.â€<sup id="fnref:Wolfram" role="doc-noteref"><a href="#fn:Wolfram" class="footnote" rel="footnote">4</a></sup> Second, <a href="http://arxiv.org/abs/2203.02155" target="_blank" rel="noopener noreferrer">as Ouyang <em>et al</em> show</a>, models like ChatGPT have been trained on additional, human-derived metadata, such as that generated by labelers who evaluated the helpfulness (or unhelpfulness) of responses to their prompts.<sup id="fnref:Ouyang" role="doc-noteref"><a href="#fn:Ouyang" class="footnote" rel="footnote">5</a></sup> While in theory LLMs remix their source data more fully than newspaper editors cutting and pasting texts wholesale, we have seen in discussions of plagiarism that these systemsâ€™ â€œprobabilistic information about how [words] combineâ€ often lead them to combine words in precisely the way they were combined in a text from their training data. This effect is so strong that <a href="http://arxiv.org/abs/2305.00118" target="_blank" rel="noopener noreferrer">a team of researchers</a> was able to query which in-copyright works are present in GPT4â€™s training data by asking ChatGPT to complete sentences from those works with character names the model could only know if that work was present in the training data.<sup id="fnref:Chang" role="doc-noteref"><a href="#fn:Chang" class="footnote" rel="footnote">6</a></sup></p> <p>This aspect of large language models points to a salient comparison with nineteenth-century newspaper reprinting, which is the fuzzy application of copyright and intellectual property to the medium. The content in US newspapers was not explicitly protected by copyright until 1909, though some publishers began affixing copyright notices to particular articles (or even whole issues) in the late nineteenth century. <a href="https://www.sup.org/books/title/?id=29452" target="_blank" rel="noopener noreferrer">As Will Slauter demonstrates</a>, however, most editors â€œunderstood that newspapers were mutually dependentâ€ and did not seek to enforce charges of plagiarism against each other.<sup id="fnref:Slauter" role="doc-noteref"><a href="#fn:Slauter" class="footnote" rel="footnote">7</a></sup> While there was a general expectation (and sometimes social pressure) to cite source papers when selecting, those citations were uneven at best, and even citations were reprinted in ways that could be unreliable, such that a mistaken citation could get picked up by the exchange system and repeated. Drawing on our reprinting data from <em>Viral Texts</em>, we can say with confidence that 1. explicit citation was not as common as injunctions from editors might lead us to believe, 2. lack of citation is never a guarantee of originality, and 3. even explicit citation is often incorrect. Here we might think of the hallucinated citations often created by ChatGPT, which similarly â€œworkâ€ when verification will prove too time consuming.</p> <p>The fuzziness of newspaper copyright, however, led to problems when material from media like books, which were protected by copyright, found their way into newspapers, where the dynamics of the exchange system would quickly lead to their wide distribution. <a href="http://www.jstor.org/stable/j.ctt4cghh7" target="_blank" rel="noopener noreferrer">As Meredith McGill has shown</a>, authors such as Nathaniel Hawthorne and Charles Dickens complained loudly about being widely read and wildly popular in the newspapers without benefiting financially, while other authors like Fanny Fern sought to take advantage of reprinting to build a reputation they could capitalize on when selling books.<sup id="fnref:McGill" role="doc-noteref"><a href="#fn:McGill" class="footnote" rel="footnote">8</a></sup> The distributed nature of newspaper exchanges made it difficult to assert even clear-cut cases of plagiarism from books, so while many texts that began in books were reprinted in newspapers, few legal actions were taken. In 2024, <a href="https://authorsguild.org/advocacy/artificial-intelligence/" target="_blank" rel="noopener noreferrer">the list of lawsuits against generative AI</a> seems to grow daily, with organizations such as the Authors Guild and the <em>New York Times</em> arguing their protected works have been used without permission as training data and can be reflected back in the modelâ€™s generative output. AI companies and proponents argue that their technology is an example of transformative use,â€ drawing on (among other things) <a href="https://law.justia.com/cases/federal/appellate-courts/ca2/13-4829/13-4829-2015-10-16.html" target="_blank" rel="noopener noreferrer">a landmark 2015 finding in favor of Google Books</a>, against the Authors Guild, which found â€œGoogleâ€™s making of a digital copy to provide a search function is a transformative use, which augments public knowledge by making available information <em>about</em> Plaintiffsâ€™ books without providing the public with a substantial substitute for matter protected by the Plaintiffsâ€™ copyright interests in the original works or derivatives of them.â€<sup id="fnref:Authors" role="doc-noteref"><a href="#fn:Authors" class="footnote" rel="footnote">9</a></sup></p> <p>By characterizing Google Booksâ€”and, as AI defenders are now arguing, services like ChatGPTâ€”as information architecture that â€œaugments public knowledge,â€ these companies echo the arguments offered for newspaper exchange in <a href="https://memory.loc.gov/cgi-bin/ampage?collId=llsl&amp;fileName=001/llsl001.db&amp;recNum=861" target="_blank" rel="noopener noreferrer">the Post Office Act of 1792</a>, which codified and underwrote the exchange system by allowing â€œevery printer of newspapersâ€ to â€œsend one paper to each and every other printer of newspapers within the United States, free of postage;â€ made mailing newspapers cheap for other purposes; and made it illegal for postal employees to interfer with newspaper mailing. These laws were advocated by, among others, the USâ€™ first Postmaster General, Benjamin Franklin, specifically to ensure that informationâ€”or, more accurately to the periodâ€”â€œintelligenceâ€â€”would circulate widely.</p> <p>The most important contrast between the nineteenth-century newspaper exchange system and twenty-first century tech systems is that their commercial contexts are inverted. The Post Office Act of 1792 ensured that the postal system would <em>not</em> be a commercially-driven enterprise. The exchange system allowed small, local publications to work collectively to gather and distribute information, and prevented government interference in that cooperative arrangement. In the late nineteenth century the growth of wire services and corporate consolidation of newspaper publication led to louder callsâ€”and eventually successful lobbyingâ€”for explicit protection of newspaper content as intellectual property. In other words, the exchange system ended in large part because newspapers became primarily corporate rather than civic media, and journalism became both a profession and a category of authorship to be protected as it became a salable commodity.</p> <p>By contrast, AI systems largely began as corporate products. AI scholar and game designer <a href="https://cohost.org/mtrc/post/5864615-you-don-t-hate-ai-y" target="_blank" rel="noopener noreferrer">Mike Cook suggests</a> that much of our collective anxiety about AI is not rooted in the technology itself but instead rooted in open access materials or protected intellectual property being repackaged for corporate profit, or being used to harm workers. Writing about responses to â€œan AI tool that could automatically colour in anime line art,â€ Cook argues, â€œwhat I sawâ€¦wasnâ€™t that they didnâ€™t like the technology itself, but rather that they were worried it would be used to put people out of work, because capitalism inevitably will see this as a route to increased profitâ€ and become â€œa weapon against us.â€<sup id="fnref:Cook" role="doc-noteref"><a href="#fn:Cook" class="footnote" rel="footnote">10</a></sup> Thinking back to the metaphor of generative AI as â€œmovable type for concepts,â€ recalls <a href="https://www.digitalhumanities.org/dhq/vol/10/3/000264/000264.html" target="_blank" rel="noopener noreferrer">Elyse Grahamâ€™s writing on â€œThe Printing Press as Metaphorâ€</a>, which highlights the commercial imperatives underlying such rhetoric:</p> <blockquote> <p>Another context that illuminates the marketplace value of the printing press metaphor is that of Silicon Valley. The distinctive incentive structures of Silicon Valleyâ€”start-up culture, venture capital financing, payment in stock options, IPOsâ€”are geared toward the rhetoric of revolutionary change. For a company seeking investors to claim that the near future will be radically changed from the present, and that this change will be predicated on the autonomous force of changing technologies, implies that the future is manageable and claims ownership of that terrain. It recommends investors to the company and consumers to its products. To claim that technological change is gradual, iterative, unpredictable, and never fully complete, and that the future of technology will depend at least as much on social and cultural factors as on machinery viewed in isolation, would obviously be a weaker sell.<sup id="fnref:Graham" role="doc-noteref"><a href="#fn:Graham" class="footnote" rel="footnote">11</a></sup>Â </p> </blockquote> <p>In reality, generative AI is more like movable type where the precise location of different characters in the drawer is obscured, or setting type in a language you do not speak fluently. The resulting composition may turn out as you expect, but there may also have been words lost in translationâ€”misheard, misspelled, or just missed. Of course, you may simply be reproducingâ€”without knowledge or attributionâ€”someone elseâ€™s work, or unknowingly perpetuating misinformation. While nineteenth-century editors sometimes commented on the veracity of the stories they selected and reprinted, they more often simply marked them as â€œgoing the rounds,â€ a signal of circulation rather than veracity, which was difficult to assure in an environment where texts could be and were adapted to every political, geographical, social, or rhetorical end. As we argue in <a href="https://manifold.umn.edu/read/viral-textuality/section/68f0f4ac-3c04-4324-ba67-cb4278081322" target="_blank" rel="noopener noreferrer">the introduction to our book, which is titled <em>Going the Rounds</em></a>, that phrase â€œsometimes licensed editors to circulate material of dubious quality while washing their hands of responsibility for its veracity.â€ We might compare this to the term â€œhallucinationsâ€ used for AI systems in 2024. While the discussion of â€œhallucinationsâ€ does acknowledge some limitations for generative AI, the phrase also displaces responsibility by positing a temporarily discombobulated subjectivity rather than a flawed technical pipeline.</p> <h3 id="iv-mixed-media-metaphors">IV. Mixed Media Metaphors</h3> <p><img style="float: left; padding-top: 20px; width:25%" src="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/compositor_20907_lg.gif?raw=true"></p> <p><img style="float: right; padding-top: 20px; width:25%" src="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/0_A%20robot%20setting%20type%20in%20a%20print%20shop,%20in%20the%20style_esrgan-v1-x2plus.png?raw=true"></p> <p><img style="float: left; padding-bottom: 20px; width:25%" src="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/0_A%20robot%20setting%20type%20in%20a%20print%20shop,%20in%20the%20style_esrgan-v1-x2plus-2.png?raw=true"></p> <p><img style="float: right; padding-bottom: 20px; width:25%" src="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/0_A%20humanoid%20robot%20setting%20type%20in%20a%20print%20shop,%20in%20_esrgan-v1-x2plus-5.png?raw=true"></p> <p>In my own robot compositor example, no combination of â€œa woodcut,â€ â€œa robot,â€ â€œsetting movable type,â€ and â€œin a historical print shopâ€ produced anything close to what my imagination wanted, but instead generated vaguely woodcut-like robots in generic workshop settings. To produce my desired â€œrobot compositorâ€ image, I instead started with a historical woodcut as the seed image, which perhaps puts us closer to the realm of pasticheâ€”or reprinting, with a differenceâ€”than â€œmovable type for images.â€</p> <p><a href="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/FKD_6YJVkAAML_N.jpeg" target="_blank" rel="noopener noreferrer"><img style="float: right; width: 50%; padding: 30px;" src="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/FKD_6YJVkAAML_N.jpeg?raw=true"></a></p> <p>As Graham argued, this â€œgradual, iterative, unpredictable, and never fully completeâ€ vision would certainly be â€œa weaker sellâ€ than â€œmovable type for concepts.â€ In many ways this image depicts the â€œmachine that will think and act, but still a machineâ€ imagined in the â€œWantedâ€”A Printerâ€ selection with which I began this piece. It also bears an uncanny resemblance to â€œThe New Steam Compositorâ€ that appeared in Andrew W. Tuerâ€™s 1884 book of printing jokes, <em>Quads for Authors, Editors, and Devils</em>, an illustration intended to satirize the rhetoric of technological progress in the late nineteenth-century, and the ways that progress would in reality be felt by laborers.</p> <p>While much of our attention in <em>Viral Texts</em> centers editors and writers, the compositorâ€”the person would reset chosen selections in type for each local newspaperâ€”helps us draw a more nuanced line between the nineteenth-century print shop and the twenty-first century language model. In <a href="https://chroniclingamerica.loc.gov/lccn/sn95063180/1858-07-01/ed-1/seq-2/" target="_blank" rel="noopener noreferrer">a selection from the <em>Freemanâ€™s Champion</em></a> (Prairie City, Kansas) the compositor is described as an automaton who â€œmust remember the impressions his eye caughtâ€ of the copy before him and â€œproceed mechanically to pick up the individual letters of which every word is composedâ€ without distraction, a daily reality which takes a physical toll and â€œbrings the compositor to a premature old age.â€ An article on â€œFast Type Settingâ€ in <a href="https://chroniclingamerica.loc.gov/lccn/sn87075001/1870-06-03/ed-1/seq-1/" target="_blank" rel="noopener noreferrer">the <em>Conservative</em> newspaper</a> of Mâ€™connelsville, Ohio recommends a series of physical habits that will help a compositor â€œnot make any false motionsâ€ and perform their duty with robotic precision and speed. An earlier piece in the <a href="https://chroniclingamerica.loc.gov/lccn/sn87062070/1836-12-09/ed-1/seq-1/" target="_blank" rel="noopener noreferrer"><em>Arkansas Advocate</em></a> (Little Rock, Arkansas Territory) outlines the â€œMiseries of a Compositor,â€ noting that â€œWe hear a good deal of the miseries of editorsâ€ but far less â€œof their humble coadjutors, the compositors.â€ This piece insists, â€œThe employment of a compositor is of a two-fold nature, mechanical and mentalâ€ and notes that the compositor â€œhas a certain number of squares expected from him as a dayâ€™s workâ€¦whether his copy be clear or obscure, legible or illegible, punctuated or not.â€</p> <p>In contrast to many of the above examples, an obituary in <a href="https://chroniclingamerica.loc.gov/lccn/sn89060060/1872-11-15/ed-1/seq-1/" target="_blank" rel="noopener noreferrer">the <em>Port Tobacco Times</em></a> for Charles W. Alcott, described as â€œa model compositor,â€ praises the fact â€œthat when a piece of manuscript of poor chirography was handed him, he had the intelligence to discover its defects, and supply any omissions which the writer may have made,â€ going on to bemoan that most â€œcompositors pursue their vocation as mere automatonsâ€”picking up type mechanically as it wereâ€”and never pausing to exercise the reasoning faculties.â€ In a widely reprinted joke, which appeared, among many other places, in the <a href="https://chroniclingamerica.loc.gov/lccn/sn86079077/1871-09-30/ed-1/seq-1/" target="_blank" rel="noopener noreferrer"><em>Opelousas Journal</em></a>, editors are lampooned for thinking typesetting required no skill and could be done without compositors, a mistake revealed in the horrendous typesetting in the selection. Here, then, the compositorâ€™s humanity and intelligence are praised, though as other pieces from the <a href="https://chroniclingamerica.loc.gov/lccn/sn84038126/1876-09-01/ed-1/seq-1/" target="_blank" rel="noopener noreferrer"><em>Benton Record</em></a>(Benton, Montana Territory) and <a href="https://chroniclingamerica.loc.gov/lccn/sn86076999/1881-06-05/ed-1/seq-3/" target="_blank" rel="noopener noreferrer"><em>Morning Appeal</em></a> (Carson City, NV) show, for many in the print industry the very phrase â€œintelligent compositorâ€ was a joke and scapegoat. The â€œintelligent compositor,â€ in these tellings, is someone who exercises too much editorial discretion and corrects copy beyond what the author would wish or the text would warrant, to the point of spoiling a textâ€™s meaning or a jokeâ€™s punchline by â€œfixingâ€ regional dialect or an orthographic pun. In this reprinted joke from the <a href="https://chroniclingamerica.loc.gov/lccn/sn84026844/1881-12-01/ed-1/seq-3/" target="_blank" rel="noopener noreferrer"><em>Wheeling Daily Intelligencer</em></a>, for instance, a woman fainted â€œdead awayâ€ reading â€œBabies are fashionable this season.â€ The mistake is â€œall the fault of the intelligence compositor,â€ who was supposed to set, â€œRubies are fashionable this season.â€</p> <p>In both twenty-first century rhetoric about artificial intelligence and nineteenth-century jokes about printers or compositors, we can trace anxiety aboutâ€”even contrasting desires forâ€”the line between â€œthe mechanical and the mental.â€ Nineteenth-century editors wanted compositors who worked tirelessly and automatically, but also compositors who exercised discretion, even ingenuity. If we want to draw a direct comparison between large language models and the print shop, the LLM is perhaps more a species of compositor than an editor, as its textual resetting is more granular, individual words and phrases rather thanâ€”usually, at leastâ€”whole snippets. Likewise if we cast a system like ChatGPT as a â€œmechanicalâ€ tool, we deflect or flatten questions about biases in its training data, intellectual property theft, or the ethics of its use in academic or professional writing. Ultimately, AI systems remix existing cultural artifacts, rather than producing new ideas. This is a common critique but also a means of locating these technologies within the continuum book historians and bibliographers study, and in fact places them in dialogue with a range of historical media that are also based in overlay, pastiche, and remix, such as commonplace books, scrapbooks, palimpsests, or of course newspaper reprinting. None of these practices are identical to the species of text reuse produced by an LLM, but the practices share family resemblances.</p> <h3 id="v-llm-analyses">V. LLM Analyses</h3> <p>In this final section, I want to suggest a few ways the family resemblances between reprinted nineteenth-century newspaper texts and large language models might facilitate scholarly analysis, both <em>of</em> and <em>with</em> LLMs. Much has been written about the lackluster output of services like ChatGPT for student (and other) writing. As models designed to produce text probabilistically, based on existing text in their training data, language models tend to generate average or mundane prose, at least thus far. I want to suggest, however, that for historians, literary scholars, and other humanities researchers, this quality of LLMs can be a boon to tasks also based on normative patterns across large-scale historical newspaper collections, such as genre classification, textual segmentation, or topic identification. As <a href="https://tedunderwood.com/2021/10/21/latent-spaces-of-culture/" target="_blank" rel="noopener noreferrer">Ted Underwood argues</a>, â€œHistorians are already in the habit of finding meaning in genres, nursery rhymes, folktale motifs, ruins, political trends, and other patterns that never had a single author with a clear purpose.â€ When considered not as objective representations of all language, but instead as highly contextual â€œmodels of culture,â€ LLMs become not unknowable oracles but instead bound, describable, and comparable cultural artifacts that we can both study and use.</p> <p>At the core of many of the debates around AI models of cultureâ€”whether text, image, audio, or videoâ€”is anxiety about corporate control, proprietary software, and â€œblack-boxâ€ systems. Such machine learning models are trained on massive amounts of data, but OpenAI and other companies typically do not disclose the exact composition of their modelsâ€™ training data, making it difficult for users to ascertain the reliability, representativeness, or limitations of the models, or their relevance to different domains. That the companies building these models wish to obscure their provenance and functions, however, does not mean those things are unavailable for study and analysis. In <a href="https://ryancordell.org/research/aibibliography/">a talk last year</a>, I discussed a series of approaches based on bibliography and data archaeology for reverse engineering AI systems.</p> <p>Throughout this talk I have suggested that historical newspaper reprinting we study in <em>Viral Texts</em> might help illuminate LLMs, and in that earlier talk I discuss how the reprint detection methods we use for tracing historical reprinting offer an approach to reverse engineering an LLM like GPT. I will not repeat that information here, but it suggests that since LLM-generated prose is, as Bender <em>et al</em> argue, â€œstitched togetherâ€ from existing prose, methods used to study historical text reuse can help use understand the seams in that stitching.</p> <p>In newer experiments, I draw on the LLM not to generate new text, but instead to compare and annotate, two of the â€œbasic functions common to scholarly activity across disciplinesâ€ that <a href="https://people.brandeis.edu/~unsworth/Kings.5-00/primitives.html" target="_blank" rel="noopener noreferrer">John Unsworth named</a> â€œscholarly primitives.â€ These are both tasks of pattern recognition that might be productively automated. In the case of <em>Viral Texts</em> our initial reprint-detection methods are largely content-agnostic, seeking only to identify passages of text that are duplicated across pages. This generates many millions of prospective reprints, which we then sift through in various ways in search of meaningful texts and patterns which we can analyze with both computational and humanistic methods. In experiments with project RAs <a href="https://manifold.umn.edu/read/untitled-bd3eb0af-fdad-4dd6-9c94-3fd15d522ab6/section/06899e82-8f06-43d2-9fc9-ea04dffef886" target="_blank" rel="noopener noreferrer">Jonathan D. Fitzgerald</a> and <a href="https://doi.org/10.1093/ahr/rhad493" target="_blank" rel="noopener noreferrer">Avery Blankenship</a>, I have experimented with methods for computational genre analysis to identify texts useful for particular studies of poetry, or fiction, or news, and so on. The scale of our data makes a computational approach to genre necessary.</p> <p>These experiments were largely supervised genre classification, where we train a model for a genre of interest by hand-tagging a corpus of texts from that genre. The model learns from the training data the linguistic or structural features of the genre, and when applied to an unknown text will assign a probability that the unknown text belongs to the trained genre. This is a well-established method for genre detection, but requires significant time and labor creating the training set and is difficult to adjust on the fly, as (for example) new genres come to light. While GPT-4 was not trained primarily on nineteenth-century newspaper writing, it was, I would argue, trained on similar, modern genres. And like the output of an LLM, the point of genre is to group similar texts together. Though scholars disagree about the precise boundaries, every genre label is an argument about patterns, similarities, and commonalities. In other words, the normative features of an LLM are precisely why they might work well for genre analysis at scale, at least so long as the genre categories are relatively broad.</p> <p><a href="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/VTGenre-GPT.R" target="_blank" rel="noopener noreferrer">I wrote a script that prompts GPT4</a> to assign a genre to each text provided from a pre-determined list of possible genres.<sup id="fnref:script" role="doc-noteref"><a href="#fn:script" class="footnote" rel="footnote">12</a></sup> I use a pre-determined list to prevent it responding in a verbose way that would be difficult to work with computationally, and also to prevent too long a list of niche or subgenres, though it could be interesting to see what genres it would assign without such a constraint. It should be said that even with these constraints, the API occasionally returned a genre not in my list, or a response with additional prose (e.g. â€œthe genre of the provided text wouldâ€) that was not requested.</p> <p>For the most part, however, in this experiment <a href="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/vt-genres-largeClusters.csv" target="_blank" rel="noopener noreferrer">GPT4â€™s labels tend to align with those a human researcher would likely make</a>. Hand-checking a list of 300 random clusters assigned genres by GPT4, I only strongly disagreed with 3 of its assignments, and I might have quibbled with another 6 or so, as texts that crossed genres, but which I would not say were wrong. As with standard genre classification experiments where you might ask more than one expert to label each text, to find points of agreement or divergence, I could imagine comparing human and LLM-derived labels, or even using multiple models and using labels where they agree. Expanding the inquiry a bit, I used GPT4 to label genre for the 1000 largest clusters in our Viral Texts data. Though we work to filter out advertisements, they inevitably leak through, so I am not surprised it was the biggest category. We might also not be surprised to see â€œpoliticalâ€ in the second spot. From here, however, we see some of the unique features of nineteenth-century newspapers, as â€œadvice,â€ â€œpoem,â€ â€œreligious,â€ â€œhumor,â€ and â€œliteraryâ€ occupy the five next positions ahead, even, of â€œnews.â€</p> <p><a href="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/1000%20top%20clusters.png" target="_blank" rel="noopener noreferrer"><img style="align: center; padding: 30px; width:100%" src="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/1000%20top%20clusters.png?raw=true"></a></p> <p>I do think these experiments benefit from the generic similarities between newspapers and the web, and similar experiments on texts more divergent from LLMsâ€™ training data would likely see less reliable results. Currently scholars are building large language models trained on period or genre-specific data, which will be better suited to these kinds of tasks. But even without fine-tuning, these results are promising. Importantly, my goal with these experiments is not to prove a theory or build a model of genre. These labels will not contribute to claims about where the line between humor and anecdote actually can or should be drawn. In the nineteenth-century paper, anecdotes are often humorous, so this line is not easily determined. Instead, my aims are practical. We have an enormous well of data about nineteenth-century newspaper reprinting, out of which we often want to identify and more closely explore particular subsets. An initial pass labeling by genre would be an enormous boon to such exploration, allowing usâ€”for instanceâ€”to more easily filter to a subset of clusters likely to be poems, fiction, jokes, and so on. These results will prompt me to experiment more with LLMs as an exploratory tool for large historical datasets. In particular, I hope to run similar experiments with models finetuned on historical newspaper data to see if they are both more reliable and more amendable to scholarly nuance. Comparison among models will be essential to this work moving forward, ideally among both corporate models and alternatives managed by non-profit or academic organizations.</p> <p>Which brings me to another, similar experiment, which takes advantage of LLMsâ€™ usefulness summarizing provided texts, as <a href="https://tedunderwood.com/tag/llms/" target="_blank" rel="noopener noreferrer">Underwood has shown</a>. The <em>Viral Texts</em> project is currently collaborating with Washington Universityâ€™s <a href="https://www.racialviolencearchive.com/" target="_blank" rel="noopener noreferrer">Racial Violence Archive</a> on <a href="https://viraltexts.org/2023/01/24/vrt/" target="_blank" rel="noopener noreferrer">â€œThe Virality of Racial Terror in US Newspapers, 1863-1921</a>, which seeks â€œto trace the the circulation of reports about anti-Black violence in US newspapers in the nineteenth and early twentieth centuries.â€ One of our initial goals is to develop methods to automatically identify whether stories in our historical newspaper data are likely to be about an incident of racial violence. In some ways this is a similar problem to genre detection, but is complicated by the fact that many historical newspaper archives, such as Chronicling America, do not divide their data by article, but instead by page, which means a report about an incident of racial violence may be nested within a much larger text that includes many other topics and genres. We are approaching this problem from several methodological anglesâ€”most of which I cannot describe todayâ€”but in one very preliminary but intriguing experiment, I used GPT4 to summarize newspaper pages, identifying any sections of text that seemed to report an incident of racial violence. In this case, I used pages from the <em>New National Era</em>, a newspaper published in Washington D.C. by J. Sella Martin and Frederick Douglass in the 1870s, advocating for reconstruction, reporting on the US Black community and its efforts toward equality, documenting continued prejudice and violence against the Black community, and sharing general news.</p> <p>GPT4 does <a href="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/VRT-answers.csv" target="_blank" rel="noopener noreferrer">a reasonably good job</a> identifying passages within larger text that may address incidents of racial violence. It identifies two such texts in the <a href="https://chroniclingamerica.loc.gov/lccn/sn84026753/1873-04-24/ed-1/seq-2/" target="_blank" rel="noopener noreferrer">24 April 1873</a> issue, for instance. It first highlights the article â€œLouisiana,â€ which describes â€œThe brutal and cowardly massacre in Grant Parish of colored men by white men in obedience to the teachings of such negro-hating journals as the New York <em>Tribune</em>.â€ Elsewhere on that same page, the article â€œShall the Doctrine be Universally Applied?â€ quotes the Boston <em>Globe</em>â€™s argument for retributive violence against Native Americans and asks whether â€œthe whole rebel populationâ€ of the South will be similarly punished for their unrelenting violence against Black Americans, which the article lists only in part. In some ways GPT4 is perhaps too broad in its interpretation of this task, selecting in other instances text a historian might categorize as discrimination, but not violence. Likely this particular task would benefit greatly from fine-tuning or a custom language model, as the subtleties of nineteenth-century newspaper language can make this information retrieval task more challenging for a model trained on the twenty-first century internet. However, if the goal for this work is to process thousands or even millions of newspaper pages and identify smaller, discrete articles worthy of closer scholarly attention by a human expert, these results are, to my mind promising.</p> <h3 id="conclusions">Conclusions</h3> <p>It is not lost on me that in this final experiment I am asking GPT-4 to act as a kind of exchanges editor: to comb through a digital pile of newspaper pages, identify and cut out texts of interest to my research, and paste them into a dataframe for further analysis. In comparing historical practices of unoriginal writing and contemporary AI, I do not seek to collapse historical distinctions or wave away concerns about the material and ethical implications of AI. Instead, I seek to cut through some of the marketing hype and position AI technologies as material-cultural phenomena which we can apprehend and critiqueâ€”and teach our students to apprehend and critiqueâ€”by adapting our existing qualitative and quantitative toolkits.</p> <p>I would argue that our path forward with large-language models and other AI models will rest in pedagogy. As communities of scholars and practioners assemble more robust understandings of these tools, their training data, and the workings of their generative algorithms, we can help students dissect those same workings, evaluate the ways in which such tools might (or might not) assist their work, and advocate for needed changes and regulation. Looking toward longer histories of unoriginal writing can help contextualize debates that seem novel and thus unknowable. Generative AI is not quite movable type for concepts, but there is a link between these systems and the printing house we can use to cut through the hype.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:Bull" role="doc-endnote"> <p>Bull, Sarah. 2023. â€œContent Generation in the Age of Mechanical Reproduction.â€ <em>Book History</em>, pg. 325Â <a href="#fnref:Bull" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:Bender" role="doc-endnote"> <p>Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. â€œOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big? &amp;#X1f99c;â€ In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 610â€“23. FAccT â€™21. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3442188.3445922" target="_blank" rel="noopener noreferrer">https://doi.org/10.1145/3442188.3445922</a>.Â <a href="#fnref:Bender" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:Chiang" role="doc-endnote"> <p>Chiang, Ted. 2023. â€œChatGPT Is a Blurry JPEG of the Web.â€ <em>The New Yorker</em>, February 9, 2023. <a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web" target="_blank" rel="noopener noreferrer">https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web</a>.Â <a href="#fnref:Chiang" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:Wolfram" role="doc-endnote"> <p>Wolfram, Stephen. 2023. â€œWhat Is ChatGPT Doing â€¦ and Why Does It Work?â€ February 14, 2023. <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/" target="_blank" rel="noopener noreferrer">https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/</a>.Â <a href="#fnref:Wolfram" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:Ouyang" role="doc-endnote"> <p>Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. â€œTraining Language Models to Follow Instructions with Human Feedback.â€ arXiv. <a href="https://doi.org/10.48550/arXiv.2203.02155" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.2203.02155</a>.Â <a href="#fnref:Ouyang" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:Chang" role="doc-endnote"> <p>Chang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023. â€œSpeak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.â€ arXiv. <a href="https://doi.org/10.48550/arXiv.2305.00118" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.2305.00118</a>.Â <a href="#fnref:Chang" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:Slauter" role="doc-endnote"> <p>Slauter, Will. 2019. <em>Who Owns the News?: A History of Copyright</em>. Stanford University Press.Â <a href="#fnref:Slauter" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:McGill" role="doc-endnote"> <p>McGill, Meredith L. 2007. <em>American Literature and the Culture of Reprinting, 1834-1853</em>. University of Pennsylvania Press.Â <a href="#fnref:McGill" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:Authors" role="doc-endnote"> <p>Authors Guild v. Google, Inc. 2015. United States Court of Appeals for the Second Circuit.Â <a href="#fnref:Authors" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:Cook" role="doc-endnote"> <p>Cook, Mike. â€œYou Donâ€™t Hate AI. You Hate Capitalism.â€ mike cook on cohost, May 11, 2024. <a href="https://cohost.org/mtrc/post/5864615-you-don-t-hate-ai-y" target="_blank" rel="noopener noreferrer">https://cohost.org/mtrc/post/5864615-you-don-t-hate-ai-y</a>.Â <a href="#fnref:Cook" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:Graham" role="doc-endnote"> <p>Graham, Elyse. â€œThe Printing Press as Metaphor.â€ Digital Humanities Quarterly 10, no. 3 (2016). <a href="http://www.digitalhumanities.org/dhq/vol/10/3/000264/000264.html" target="_blank" rel="noopener noreferrer">http://www.digitalhumanities.org/dhq/vol/10/3/000264/000264.html</a>.Â <a href="#fnref:Graham" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> <li id="fn:script" role="doc-endnote"> <p>Note that this script is pretty rough-and-ready, and assumes you are working with data in the format of our Viral Texts clusters. It also requires <a href="https://openai.com/index/openai-api/" target="_blank" rel="noopener noreferrer">an OpenAI API key</a>. Iâ€™ve added <a href="https://github.com/rccordell/rccordell.github.io/blob/master/img/pastepot/large-clusters.csv" target="_blank" rel="noopener noreferrer">a spreadsheet of our larger clusters</a> so readers can see the typical format and try to reproduce these results. Also note that the <code class="language-plaintext highlighter-rouge">Sys.sleep</code> commands in the loops here are important, as they introduce a pause in between API requests, so that you donâ€™t flood OpenAIâ€™s servers and get kicked off.Â <a href="#fnref:script" class="reversefootnote" role="doc-backlink">â†©</a></p> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2025 Ryan C. Cordell. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>